import librosa
import torch
import logging
from transformers import Wav2Vec2ForCTC, AutoProcessor, Wav2Vec2ForSequenceClassification, AutoFeatureExtractor
import time
import numpy as np
from collections import Counter
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple, Optional
from scipy.signal import correlate
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw

logger = logging.getLogger(__name__)

# --- CONFIGURATION ---
MODEL_ID = "facebook/mms-1b-all"
LID_MODEL_ID = "facebook/mms-lid-126"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

INDIAN_LANGUAGES = {
    'hindi': 'hin', 'english': 'eng', 'tamil': 'tam', 'telugu': 'tel',
    'bengali': 'ben', 'marathi': 'mar', 'gujarati': 'guj', 'kannada': 'kan',
    'malayalam': 'mal', 'punjabi': 'pan', 'urdu': 'urd', 'assamese': 'asm',
    'odia': 'ory', 'bhojpuri': 'bho', 'maithili': 'mai'
}

# Research-based thresholds (from literature review)
PROLONGATION_CORRELATION_THRESHOLD = 0.90  # >0.9 spectral correlation
PROLONGATION_MIN_DURATION = 0.25  # >250ms (Revisiting Rule-Based Stuttering Detection, 2025)
BLOCK_SILENCE_THRESHOLD = 0.35  # >350ms silence (multiple papers)
REPETITION_DTW_THRESHOLD = 0.15  # Dynamic Time Warping distance threshold
SPEECH_RATE_MIN = 2.0  # syllables per second (lower bound)
SPEECH_RATE_MAX = 6.0  # syllables per second (upper bound)

@dataclass
class StutterEvent:
    type: str  # 'repetition', 'prolongation', 'block'
    start: float
    end: float
    text: str
    confidence: float
    acoustic_features: Dict[str, float] = None

class StutterDetector:
    """
    Research-Based Stutter Detection Engine
    
    Based on recent literature (2023-2025):
    - MFCC-based acoustic feature extraction
    - Spectral correlation for prolongation detection (>0.9, >250ms)
    - DTW for repetition detection
    - Speaking-rate normalized thresholds
    - Multi-layer detection cascade
    
    References:
    - Apple SEP-28k Dataset (2021)
    - Revisiting Rule-Based Stuttering Detection (ArXiv 2025)
    - MDPI Applied Sciences Stuttering Detection (2023)
    """
    
    def __init__(self):
        logger.info(f"üöÄ Initializing Research-Based AI Engine on {DEVICE}...")
        try:
            self.processor = AutoProcessor.from_pretrained(MODEL_ID)
            self.model = Wav2Vec2ForCTC.from_pretrained(
                MODEL_ID, 
                torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
                target_lang="eng",
                ignore_mismatched_sizes=True
            ).to(DEVICE)
            self.model.eval()
            self.loaded_adapters = set()
            self._init_common_adapters()
            logger.info("‚úÖ Engine Online - Research-Based Algorithm Loaded")
        except Exception as e:
            logger.error(f"üî• Engine Failure: {e}")
            raise

    def _init_common_adapters(self):
        for code in ['eng', 'hin']:
            try:
                self.model.load_adapter(code)
                self.loaded_adapters.add(code)
            except: pass

    # --- CORE PIPELINE ---

    def analyze_audio(self, audio_path: str, language: str = 'english') -> dict:
        start_time = time.time()
        
        # 1. Language Setup
        if language == 'auto':
            lang_code = self._detect_language_robust(audio_path)
        else:
            lang_code = INDIAN_LANGUAGES.get(language.lower(), 'eng')
            
        self._activate_adapter(lang_code)
        
        # 2. Audio Processing & Feature Extraction
        audio, sr = librosa.load(audio_path, sr=16000)
        duration = librosa.get_duration(y=audio, sr=sr)
        
        # MFCCs for spectral analysis
        mfcc_features = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20, hop_length=320, n_fft=512)
        
        # Acoustic features for Block detection
        rms_energy = librosa.feature.rms(y=audio, hop_length=320)[0]
        
        # 3. Speaking Rate Estimation (Adaptive Baseline)
        speaking_rate = self._estimate_speaking_rate(audio, sr)
        
        # 4. Wav2Vec2 Inference (Get Logits AND Probabilities)
        inputs = self.processor(audio, sampling_rate=16000, return_tensors="pt")
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
        if DEVICE == "cuda":
            inputs["input_values"] = inputs["input_values"].half()

        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits # Raw scores
        
        # --- NEW: Calculate Acoustic Uncertainty (Entropy) ---
        # Instead of hardcoding 0.0, we calculate how "confused" the model was.
        # High entropy = The audio was unclear or stuttered.
        entropy_score, low_conf_tokens = self._calculate_uncertainty(logits)

        predicted_ids = torch.argmax(logits, dim=-1)
        transcript = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        
        # 5. Extract Timestamps
        word_timestamps = self._extract_word_timestamps(predicted_ids[0], logits[0], transcript)
        
        # 6. Multi-Layer Detection (Research Algorithms)
        events = []
        
        # Layer A: Spectral Prolongation
        events.extend(self._detect_prolongations_spectral(mfcc_features, speaking_rate, word_timestamps))
        
        # Layer B: Silence Blocks
        events.extend(self._detect_blocks_research(audio, sr, rms_energy, word_timestamps, speaking_rate))
        
        # Layer C: DTW Repetitions 
        events.extend(self._detect_repetitions_dtw(mfcc_features, word_timestamps, transcript, speaking_rate))

        # 7. Merge & Metrics
        cleaned_events = self._deduplicate_events_cascade(events)
        metrics = self._calculate_metrics_research(cleaned_events, duration, speaking_rate)

        return {
            'actual_transcript': transcript,
            'target_transcript': transcript, # In blind ASR, target is actual.
            'mismatched_chars': low_conf_tokens, # NO LONGER HARDCODED []
            'mismatch_percentage': metrics['severity_score'],
            'ctc_loss_score': round(entropy_score, 4), # NO LONGER HARDCODED 0.0
            'stutter_timestamps': [self._event_to_dict(e) for e in cleaned_events],
            'total_stutter_duration': metrics['total_duration'],
            'stutter_frequency': metrics['frequency'],
            'severity': metrics['severity_label'],
            'confidence_score': metrics['confidence'],
            'speaking_rate_sps': round(speaking_rate, 2),
            'analysis_duration_seconds': round(time.time() - start_time, 2),
            'model_version': f'research-mms-1b-{lang_code}',
            'algorithm': 'spectral_correlation_dtw_v2.1'
        }

    # --- NEW HELPER METHOD FOR UNCERTAINTY ---

    def _calculate_uncertainty(self, logits):
        """
        Calculates Entropy to replace 'CTC Loss'.
        Returns:
            - Global Entropy Score (Float): The overall 'confusion' of the model.
            - Low Confidence Tokens (List): Specific parts of speech that were garbled.
        """
        # Convert logits to probabilities (0.0 to 1.0)
        probs = torch.nn.functional.softmax(logits, dim=-1)
        
        # 1. Calculate Entropy: -sum(p * log(p))
        # This measures the "chaos" in the prediction distribution
        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)
        
        # Global "Loss" Score (Average Entropy across the file)
        avg_entropy = torch.mean(entropy).item()
        
        # 2. Find Low Confidence Areas (Mismatches)
        # We look for spikes in entropy that exceed the mean + 1 std dev
        max_probs, _ = torch.max(probs, dim=-1)
        threshold = 0.40 # If confidence is below 40%, it's likely a mismatch/stumble
        
        low_conf_indices = torch.where(max_probs < threshold)[1] # Get time indices
        
        # Group indices into regions (simple clustering)
        mismatches = []
        if len(low_conf_indices) > 0:
            # We just take a sample of timestamps where the model struggled
            # In a real app, you'd map these time-indices back to characters
            timestamps = low_conf_indices.float() * 0.02 # Convert frame to seconds
            
            # Simple clustering to avoid returning 1000 items
            last_t = -1
            for t in timestamps:
                if t - last_t > 0.5: # New event if gap > 0.5s
                    mismatches.append({"time": round(t.item(), 2), "type": "low_confidence"})
                last_t = t.item()
                
        return avg_entropy, mismatches

    # --- RESEARCH-BASED DETECTION ALGORITHMS ---

    def _detect_prolongations_spectral(
        self, 
        mfcc_features: np.ndarray,
        speaking_rate: float,
        word_timestamps: List[Dict]
    ) -> List[StutterEvent]:
        """
        Prolongation detection using frame-to-frame spectral correlation
        
        Research basis: "Revisiting Rule-Based Stuttering Detection" (2025)
        - Correlation threshold: >0.9
        - Duration threshold: >250ms (adaptive based on speaking rate)
        """
        events = []
        frame_time = 0.02  # 20ms frames
        
        # Adaptive threshold based on speaking rate
        min_duration = PROLONGATION_MIN_DURATION * (SPEECH_RATE_MIN / max(speaking_rate, 1.0))
        min_frames = int(min_duration / frame_time)
        
        n_frames = mfcc_features.shape[1]
        prolongation_start = None
        high_correlation_frames = 0
        
        for i in range(1, n_frames):
            # Compute correlation between consecutive frames
            corr = np.corrcoef(mfcc_features[:, i-1], mfcc_features[:, i])[0, 1]
            
            if corr > PROLONGATION_CORRELATION_THRESHOLD:
                if prolongation_start is None:
                    prolongation_start = i - 1
                high_correlation_frames += 1
            else:
                # End of potential prolongation
                if prolongation_start is not None and high_correlation_frames >= min_frames:
                    start_time = prolongation_start * frame_time
                    end_time = i * frame_time
                    
                    # Match to word context
                    word_context = self._find_word_at_time(start_time, word_timestamps)
                    
                    events.append(StutterEvent(
                        type='prolongation',
                        start=round(start_time, 2),
                        end=round(end_time, 2),
                        text=word_context if word_context else '<prolongation>',
                        confidence=min(corr, 0.98),
                        acoustic_features={'correlation': float(corr), 'duration': end_time - start_time}
                    ))
                
                prolongation_start = None
                high_correlation_frames = 0
        
        return events

    def _detect_blocks_research(
        self,
        audio: np.ndarray,
        sr: int,
        rms_energy: np.ndarray,
        word_timestamps: List[Dict],
        speaking_rate: float
    ) -> List[StutterEvent]:
        """
        Block detection using silence analysis with adaptive thresholds
        
        Research basis: Multiple papers (2023-2025)
        - Silent blocks: >350ms silence
        - Adaptive to speaking rate
        """
        events = []
        frame_time = 0.02
        
        # Adaptive threshold: faster speakers = shorter threshold
        block_threshold = BLOCK_SILENCE_THRESHOLD * (SPEECH_RATE_MIN / max(speaking_rate, 1.0))
        
        # Energy-based silence detection
        silence_threshold = np.percentile(rms_energy, 10)  # Bottom 10% = silence
        
        is_silent = rms_energy < silence_threshold
        silence_start = None
        silence_duration = 0
        
        for i, silent in enumerate(is_silent):
            if silent:
                if silence_start is None:
                    silence_start = i
                silence_duration += 1
            else:
                if silence_start is not None:
                    duration_sec = silence_duration * frame_time
                    
                    if duration_sec > block_threshold:
                        start_time = silence_start * frame_time
                        end_time = i * frame_time
                        
                        # Check if this gap is between words (not natural pause)
                        if self._is_unnatural_gap(start_time, end_time, word_timestamps):
                            events.append(StutterEvent(
                                type='block',
                                start=round(start_time, 2),
                                end=round(end_time, 2),
                                text='<silence>',
                                confidence=0.90,
                                acoustic_features={'duration': duration_sec, 'silence_ratio': float(silence_threshold)}
                            ))
                
                silence_start = None
                silence_duration = 0
        
        return events

    def _detect_repetitions_dtw(
        self,
        mfcc_features: np.ndarray,
        word_timestamps: List[Dict],
        transcript: str,
        speaking_rate: float
    ) -> List[StutterEvent]:
        """
        Repetition detection using Dynamic Time Warping
        
        Research basis: Multiple papers using DTW for stuttering detection
        - Detects similar acoustic patterns
        - Word and syllable level repetitions
        """
        events = []
        
        if len(word_timestamps) < 2:
            return events
        
        # Adaptive DTW threshold based on speaking rate
        dtw_threshold = REPETITION_DTW_THRESHOLD * (max(speaking_rate, 1.0) / SPEECH_RATE_MIN)
        
        # Check consecutive words for repetition
        for i in range(len(word_timestamps) - 1):
            curr_word = word_timestamps[i]
            next_word = word_timestamps[i + 1]
            
            # Text-based check first (fast)
            if curr_word['word'].lower() == next_word['word'].lower():
                # Extract MFCC segments
                curr_start_frame = int(curr_word['start'] / 0.02)
                curr_end_frame = int(curr_word['end'] / 0.02)
                next_start_frame = int(next_word['start'] / 0.02)
                next_end_frame = int(next_word['end'] / 0.02)
                
                if curr_end_frame > mfcc_features.shape[1] or next_end_frame > mfcc_features.shape[1]:
                    continue
                
                curr_segment = mfcc_features[:, curr_start_frame:curr_end_frame].T
                next_segment = mfcc_features[:, next_start_frame:next_end_frame].T
                
                # Compute DTW distance
                try:
                    distance, _ = fastdtw(curr_segment, next_segment, dist=euclidean)
                    normalized_distance = distance / max(len(curr_segment), len(next_segment))
                    
                    if normalized_distance < dtw_threshold:
                        events.append(StutterEvent(
                            type='repetition',
                            start=round(curr_word['start'], 2),
                            end=round(next_word['end'], 2),
                            text=curr_word['word'],
                            confidence=min(0.95, 1.0 - normalized_distance),
                            acoustic_features={'dtw_distance': float(normalized_distance)}
                        ))
                except:
                    pass
        
        return events

    # --- HELPER FUNCTIONS ---

    def _estimate_speaking_rate(self, audio: np.ndarray, sr: int) -> float:
        """
        Estimate speaking rate in syllables per second
        Uses onset detection as proxy for syllable nuclei
        """
        onset_frames = librosa.onset.onset_detect(y=audio, sr=sr, hop_length=320)
        duration = len(audio) / sr
        
        if duration > 0:
            rate = len(onset_frames) / duration
            # Clamp to reasonable bounds
            return max(SPEECH_RATE_MIN, min(rate, SPEECH_RATE_MAX))
        return 4.0  # Default: 4 syllables/sec

    def _extract_word_timestamps(
        self, 
        predicted_ids: torch.Tensor,
        logits: torch.Tensor,
        transcript: str
    ) -> List[Dict]:
        """Extract word-level timestamps from CTC output"""
        word_timestamps = []
        frame_time = 0.02
        
        vocab = self.processor.tokenizer.get_vocab()
        pad_token_id = self.processor.tokenizer.pad_token_id
        blank_token_id = vocab.get('[PAD]', vocab.get('<pad>', 0))
        
        current_word = ""
        word_start = None
        
        for i, token_id in enumerate(predicted_ids):
            token_id_item = token_id.item()
            
            if token_id_item != pad_token_id and token_id_item != blank_token_id:
                char = self.processor.tokenizer.decode([token_id_item])
                
                if char.strip() and char not in [' ', ',', '.', '!', '?', '|']:
                    if word_start is None:
                        word_start = i
                    current_word += char
                else:
                    if current_word and word_start is not None:
                        word_timestamps.append({
                            'word': current_word,
                            'start': word_start * frame_time,
                            'end': i * frame_time
                        })
                        current_word = ""
                        word_start = None
        
        # Add last word
        if current_word and word_start is not None:
            word_timestamps.append({
                'word': current_word,
                'start': word_start * frame_time,
                'end': (len(predicted_ids) - 1) * frame_time
            })
        
        return word_timestamps

    def _find_word_at_time(self, time: float, word_timestamps: List[Dict]) -> str:
        """Find word at given timestamp"""
        for word in word_timestamps:
            if word['start'] <= time <= word['end']:
                return word['word']
        return ""

    def _is_unnatural_gap(
        self, 
        gap_start: float, 
        gap_end: float, 
        word_timestamps: List[Dict]
    ) -> bool:
        """Check if gap is unnatural (mid-utterance vs natural pause)"""
        # Find words around the gap
        before_words = [w for w in word_timestamps if w['end'] <= gap_start]
        after_words = [w for w in word_timestamps if w['start'] >= gap_end]
        
        # If gap is between words (not at utterance boundaries), it's suspicious
        return len(before_words) > 0 and len(after_words) > 0

    def _deduplicate_events_cascade(self, events: List[StutterEvent]) -> List[StutterEvent]:
        """
        Merge overlapping events with precedence hierarchy
        Precedence: Block > Repetition > Prolongation (research-based)
        """
        if not events: return []
        
        # Sort by type priority, then by start time
        priority = {'block': 0, 'repetition': 1, 'prolongation': 2}
        events.sort(key=lambda x: (x.start, priority.get(x.type, 3)))
        
        merged = []
        current = events[0]
        
        for next_event in events[1:]:
            # Check overlap
            if next_event.start < current.end:
                # Keep higher priority event, extend duration
                if priority.get(next_event.type, 3) < priority.get(current.type, 3):
                    current = next_event
                current.end = max(current.end, next_event.end)
            else:
                merged.append(current)
                current = next_event
        
        merged.append(current)
        return merged

    def _calculate_metrics_research(
        self, 
        events: List[StutterEvent], 
        duration: float,
        speaking_rate: float
    ) -> Dict:
        """Calculate metrics based on clinical research"""
        total_time = sum(e.end - e.start for e in events)
        freq = (len(events) / duration * 60) if duration > 0 else 0
        ratio = (total_time / duration * 100) if duration > 0 else 0
        
        # Severity classification (based on clinical standards)
        if ratio < 2: 
            label = 'none'
        elif ratio < 5:
            label = 'mild'
        elif ratio < 10:
            label = 'moderate'
        elif ratio < 20:
            label = 'severe'
        else:
            label = 'very_severe'
        
        # Confidence weighted by number of detections and acoustic features
        avg_confidence = np.mean([e.confidence for e in events]) if events else 0.5
        
        return {
            'total_duration': round(total_time, 2),
            'frequency': round(freq, 1),
            'severity_score': round(ratio, 1),
            'severity_label': label,
            'confidence': round(avg_confidence, 2),
            'num_blocks': sum(1 for e in events if e.type == 'block'),
            'num_prolongations': sum(1 for e in events if e.type == 'prolongation'),
            'num_repetitions': sum(1 for e in events if e.type == 'repetition')
        }

    def _event_to_dict(self, event: StutterEvent) -> Dict:
        """Convert StutterEvent to dictionary"""
        return {
            'type': event.type,
            'start': event.start,
            'end': event.end,
            'text': event.text,
            'confidence': event.confidence,
            'acoustic_features': event.acoustic_features or {}
        }

    def _detect_language_robust(self, audio_path):
        """Multi-segment voting system for LID"""
        logger.info("üïµÔ∏è Pro-LID: Analyzing audio structure...")
        try:
            lid_processor = AutoFeatureExtractor.from_pretrained(LID_MODEL_ID)
            lid_model = Wav2Vec2ForSequenceClassification.from_pretrained(LID_MODEL_ID).to(DEVICE)
            
            duration = librosa.get_duration(path=audio_path)
            segments = [(0, 10), (max(0, duration/2-5), 10), (max(0, duration-10), 10)]
            
            votes = []
            for off, dur in segments:
                if duration < off: continue
                try:
                    audio, _ = librosa.load(audio_path, sr=16000, offset=off, duration=dur)
                    if len(audio) < 16000: continue
                    
                    inputs = lid_processor(audio, sampling_rate=16000, return_tensors="pt").to(DEVICE)
                    with torch.no_grad():
                        logits = lid_model(**inputs).logits
                    votes.append(lid_model.config.id2label[torch.argmax(logits).item()])
                except: pass
            
            del lid_model
            if DEVICE == "cuda": torch.cuda.empty_cache()
            
            if not votes: return 'eng'
            final = Counter(votes).most_common(1)[0][0]
            logger.info(f"‚úÖ Detected: {final} (Votes: {votes})")
            return final
            
        except Exception as e:
            logger.error(f"LID Error: {e}")
            return 'eng'

    def _activate_adapter(self, code):
        if code not in self.loaded_adapters:
            try:
                self.model.load_adapter(code)
                self.loaded_adapters.add(code)
            except:
                logger.warning(f"Adapter {code} failed, using fallback")
                code = 'eng'
        self.processor.tokenizer.set_target_lang(code)
        self.model.load_adapter(code)