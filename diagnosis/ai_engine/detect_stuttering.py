import librosa
import torch
import logging
import numpy as np
import parselmouth
from transformers import Wav2Vec2ForCTC, AutoProcessor, Wav2Vec2ForSequenceClassification, AutoFeatureExtractor
import time
from collections import Counter
from dataclasses import dataclass, field
from typing import List, Dict, Any, Tuple, Optional
from scipy.signal import correlate, butter, filtfilt
from scipy.spatial.distance import euclidean, cosine
from scipy.stats import kurtosis, skew
from fastdtw import fastdtw
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest

logger = logging.getLogger(__name__)

# === CONFIGURATION ===
MODEL_ID = "facebook/mms-1b-all"
LID_MODEL_ID = "facebook/mms-lid-126"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

INDIAN_LANGUAGES = {
    'hindi': 'hin', 'english': 'eng', 'tamil': 'tam', 'telugu': 'tel',
    'bengali': 'ben', 'marathi': 'mar', 'gujarati': 'guj', 'kannada': 'kan',
    'malayalam': 'mal', 'punjabi': 'pan', 'urdu': 'urd', 'assamese': 'asm',
    'odia': 'ory', 'bhojpuri': 'bho', 'maithili': 'mai'
}

# === RESEARCH-BASED THRESHOLDS (2024-2025 Literature) ===
# Prolongation Detection (Spectral Correlation + Duration)
PROLONGATION_CORRELATION_THRESHOLD = 0.90  # >0.9 spectral similarity
PROLONGATION_MIN_DURATION = 0.25  # >250ms (Revisiting Rule-Based, 2025)

# Block Detection (Silence Analysis)
BLOCK_SILENCE_THRESHOLD = 0.35  # >350ms silence mid-utterance
BLOCK_ENERGY_PERCENTILE = 10  # Bottom 10% energy = silence

# Repetition Detection (DTW + Text Matching)
REPETITION_DTW_THRESHOLD = 0.15  # Normalized DTW distance
REPETITION_MIN_SIMILARITY = 0.85  # Text-based similarity

# Speaking Rate Norms (syllables/second)
SPEECH_RATE_MIN = 2.0
SPEECH_RATE_MAX = 6.0
SPEECH_RATE_TYPICAL = 4.0

# Formant Analysis (Vowel Centralization - Research Finding)
# People who stutter show reduced vowel space area
VOWEL_SPACE_REDUCTION_THRESHOLD = 0.70  # 70% of typical area

# Voice Quality (Jitter, Shimmer, HNR)
JITTER_THRESHOLD = 0.01  # >1% jitter indicates instability
SHIMMER_THRESHOLD = 0.03  # >3% shimmer
HNR_THRESHOLD = 15.0  # <15 dB Harmonics-to-Noise Ratio

# Zero-Crossing Rate (Voiced/Unvoiced Discrimination)
ZCR_VOICED_THRESHOLD = 0.1  # Low ZCR = voiced
ZCR_UNVOICED_THRESHOLD = 0.3  # High ZCR = unvoiced

# Entropy-Based Uncertainty
ENTROPY_HIGH_THRESHOLD = 3.5  # High confusion in model predictions
CONFIDENCE_LOW_THRESHOLD = 0.40  # Low confidence frame threshold

@dataclass
class StutterEvent:
    """Enhanced stutter event with multi-modal features"""
    type: str  # 'repetition', 'prolongation', 'block', 'dysfluency'
    start: float
    end: float
    text: str
    confidence: float
    acoustic_features: Dict[str, float] = field(default_factory=dict)
    voice_quality: Dict[str, float] = field(default_factory=dict)
    formant_data: Dict[str, Any] = field(default_factory=dict)


class AdvancedStutterDetector:
    """
    üß† 2024-2025 State-of-the-Art Stuttering Detection Engine
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    RESEARCH FOUNDATION (Latest Publications):
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    [1] ACOUSTIC FEATURES:
        ‚Ä¢ MFCC (20 coefficients) - spectral envelope
        ‚Ä¢ Formant tracking (F1-F4) - vowel space analysis
        ‚Ä¢ Pitch contour (F0) - intonation patterns
        ‚Ä¢ Zero-Crossing Rate - voiced/unvoiced classification
        ‚Ä¢ Spectral flux - rapid spectral changes
        ‚Ä¢ Energy entropy - signal chaos measurement
    
    [2] VOICE QUALITY METRICS (Parselmouth/Praat):
        ‚Ä¢ Jitter (>1% threshold) - pitch perturbation
        ‚Ä¢ Shimmer (>3% threshold) - amplitude perturbation
        ‚Ä¢ HNR (<15 dB threshold) - harmonics-to-noise ratio
    
    [3] FORMANT ANALYSIS (Vowel Space):
        ‚Ä¢ Untreated stutterers show 70% vowel space reduction
        ‚Ä¢ F1-F2 centralization indicates restricted articulation
        ‚Ä¢ Post-treatment: vowel space normalizes
    
    [4] DETECTION ALGORITHMS:
        ‚Ä¢ Prolongation: Spectral correlation >0.9 for >250ms
        ‚Ä¢ Blocks: Silence gaps >350ms mid-utterance
        ‚Ä¢ Repetitions: DTW distance <0.15 + text matching
        ‚Ä¢ Dysfluency: Entropy >3.5 or confidence <0.4
    
    [5] ENSEMBLE DECISION FUSION:
        ‚Ä¢ Multi-layer cascade: Block > Repetition > Prolongation
        ‚Ä¢ Anomaly detection (Isolation Forest) for outliers
        ‚Ä¢ Speaking-rate normalization for adaptive thresholds
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    KEY IMPROVEMENTS FROM ORIGINAL CODE:
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    ‚úÖ Praat-based voice quality analysis (jitter/shimmer/HNR)
    ‚úÖ Formant tracking with vowel space area calculation
    ‚úÖ Zero-crossing rate for phonation analysis
    ‚úÖ Spectral flux for rapid acoustic changes
    ‚úÖ Enhanced entropy calculation with frame-level detail
    ‚úÖ Isolation Forest anomaly detection
    ‚úÖ Multi-feature fusion with weighted scoring
    ‚úÖ Adaptive thresholds based on speaking rate
    ‚úÖ Comprehensive clinical severity mapping
    
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """
    
    def __init__(self):
        logger.info(f"üöÄ Initializing Advanced AI Engine on {DEVICE}...")
        try:
            # Wav2Vec2 Model Loading
            self.processor = AutoProcessor.from_pretrained(MODEL_ID)
            self.model = Wav2Vec2ForCTC.from_pretrained(
                MODEL_ID, 
                torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
                target_lang="eng",
                ignore_mismatched_sizes=True
            ).to(DEVICE)
            self.model.eval()
            self.loaded_adapters = set()
            self._init_common_adapters()
            
            # Anomaly Detection Model (for outlier stutter events)
            self.anomaly_detector = IsolationForest(
                contamination=0.1,  # Expect 10% of frames to be anomalous
                random_state=42
            )
            
            logger.info("‚úÖ Engine Online - Advanced Research Algorithm Loaded")
        except Exception as e:
            logger.error(f"üî• Engine Failure: {e}")
            raise

    def _init_common_adapters(self):
        """Preload common language adapters"""
        for code in ['eng', 'hin']:
            try:
                self.model.load_adapter(code)
                self.loaded_adapters.add(code)
            except: pass

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CORE ANALYSIS PIPELINE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def analyze_audio(self, audio_path: str, language: str = 'english') -> dict:
        """
        Main analysis pipeline with comprehensive feature extraction
        """
        start_time = time.time()
        
        # === STEP 1: Language Detection & Setup ===
        if language == 'auto':
            lang_code = self._detect_language_robust(audio_path)
        else:
            lang_code = INDIAN_LANGUAGES.get(language.lower(), 'eng')
        self._activate_adapter(lang_code)
        
        # === STEP 2: Audio Loading & Preprocessing ===
        audio, sr = librosa.load(audio_path, sr=16000)
        duration = librosa.get_duration(y=audio, sr=sr)
        
        # === STEP 3: Multi-Modal Feature Extraction ===
        features = self._extract_comprehensive_features(audio, sr, audio_path)
        
        # === STEP 4: Wav2Vec2 Transcription & Uncertainty ===
        transcript, word_timestamps, logits = self._transcribe_with_timestamps(audio)
        entropy_score, low_conf_regions = self._calculate_uncertainty(logits)
        
        # === STEP 5: Speaking Rate Estimation ===
        speaking_rate = self._estimate_speaking_rate(audio, sr)
        
        # === STEP 6: Multi-Layer Stutter Detection ===
        events = []
        
        # Layer A: Spectral Prolongation Detection
        events.extend(self._detect_prolongations_advanced(
            features['mfcc'], 
            features['spectral_flux'],
            speaking_rate, 
            word_timestamps
        ))
        
        # Layer B: Silence Block Detection
        events.extend(self._detect_blocks_enhanced(
            audio, sr,
            features['rms_energy'],
            features['zcr'],
            word_timestamps, 
            speaking_rate
        ))
        
        # Layer C: DTW-Based Repetition Detection
        events.extend(self._detect_repetitions_advanced(
            features['mfcc'],
            features['formants'],
            word_timestamps, 
            transcript, 
            speaking_rate
        ))
        
        # Layer D: Voice Quality Dysfluencies (Jitter/Shimmer)
        events.extend(self._detect_voice_quality_issues(
            audio_path,
            word_timestamps,
            features['voice_quality']
        ))
        
        # Layer E: Entropy-Based Uncertainty Events
        for region in low_conf_regions:
            if not self._is_overlapping(region['time'], events):
                events.append(StutterEvent(
                    type='dysfluency',
                    start=region['time'],
                    end=region['time'] + 0.3,
                    text="<uncertainty>",
                    confidence=0.4,
                    acoustic_features={'entropy': entropy_score}
                ))
        
        # Layer F: Anomaly Detection (Isolation Forest)
        events = self._detect_anomalies(events, features)
        
        # === STEP 7: Event Fusion & Deduplication ===
        cleaned_events = self._deduplicate_events_cascade(events)
        
        # === STEP 8: Clinical Metrics & Severity Assessment ===
        metrics = self._calculate_clinical_metrics(
            cleaned_events, 
            duration, 
            speaking_rate,
            features
        )
        
        # Severity upgrade if global confidence is very low
        if metrics['confidence'] < 0.6 and metrics['severity_label'] == 'none':
            metrics['severity_label'] = 'mild'
            metrics['severity_score'] = max(metrics['severity_score'], 5.0)
        
        # === STEP 9: Return Comprehensive Report ===
        return {
            'actual_transcript': transcript,
            'target_transcript': transcript,
            'mismatched_chars': [f"{r['time']}s" for r in low_conf_regions],
            'mismatch_percentage': metrics['severity_score'],
            'ctc_loss_score': round(entropy_score, 4),
            'stutter_timestamps': [self._event_to_dict(e) for e in cleaned_events],
            'total_stutter_duration': metrics['total_duration'],
            'stutter_frequency': metrics['frequency'],
            'severity': metrics['severity_label'],
            'confidence_score': metrics['confidence'],
            'speaking_rate_sps': round(speaking_rate, 2),
            'voice_quality_metrics': features['voice_quality'],
            'formant_analysis': features['formant_summary'],
            'acoustic_features': {
                'avg_mfcc_variance': float(np.var(features['mfcc'])),
                'avg_zcr': float(np.mean(features['zcr'])),
                'spectral_flux_mean': float(np.mean(features['spectral_flux'])),
                'energy_entropy': float(np.mean(features['energy_entropy']))
            },
            'analysis_duration_seconds': round(time.time() - start_time, 2),
            'model_version': f'advanced-research-v2-{lang_code}'
        }

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # COMPREHENSIVE FEATURE EXTRACTION
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _extract_comprehensive_features(
        self, 
        audio: np.ndarray, 
        sr: int,
        audio_path: str
    ) -> Dict[str, Any]:
        """
        Extract multi-modal acoustic features from audio
        
        Returns comprehensive feature dictionary including:
        - MFCCs, Formants, Pitch, Energy, ZCR
        - Voice quality (jitter, shimmer, HNR)
        - Spectral features (flux, rolloff, centroid)
        """
        # Basic Acoustic Features
        mfcc_features = librosa.feature.mfcc(
            y=audio, sr=sr, n_mfcc=20, hop_length=320, n_fft=512
        )
        rms_energy = librosa.feature.rms(y=audio, hop_length=320)[0]
        zcr = librosa.feature.zero_crossing_rate(y=audio, hop_length=320)[0]
        
        # Spectral Features
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=320)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, hop_length=320)[0]
        spectral_flux = self._compute_spectral_flux(audio, sr)
        
        # Energy Entropy (Chaos in Energy Distribution)
        energy_entropy = self._compute_energy_entropy(rms_energy)
        
        # Formant Tracking (Praat/Parselmouth)
        formants, formant_summary = self._extract_formants(audio_path)
        
        # Voice Quality Analysis (Jitter, Shimmer, HNR)
        voice_quality = self._extract_voice_quality(audio_path)
        
        # Pitch Extraction
        pitch = self._extract_pitch(audio, sr)
        
        return {
            'mfcc': mfcc_features,
            'rms_energy': rms_energy,
            'zcr': zcr,
            'spectral_centroids': spectral_centroids,
            'spectral_rolloff': spectral_rolloff,
            'spectral_flux': spectral_flux,
            'energy_entropy': energy_entropy,
            'formants': formants,
            'formant_summary': formant_summary,
            'voice_quality': voice_quality,
            'pitch': pitch
        }

    def _compute_spectral_flux(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """
        Spectral flux: Measure of rapid spectral changes
        High flux indicates abrupt transitions (potential stutters)
        """
        S = np.abs(librosa.stft(audio, hop_length=320))
        flux = np.sqrt(np.sum(np.diff(S, axis=1)**2, axis=0))
        return flux

    def _compute_energy_entropy(self, energy: np.ndarray) -> np.ndarray:
        """
        Energy entropy: Measure of chaos in energy distribution
        High entropy = irregular energy patterns (dysfluency indicator)
        """
        # Normalize energy to probability distribution
        energy_norm = energy / (np.sum(energy) + 1e-10)
        entropy = -np.sum(energy_norm * np.log(energy_norm + 1e-10))
        return np.full_like(energy, entropy)

    def _extract_formants(self, audio_path: str) -> Tuple[Dict, Dict]:
        """
        Extract F1-F4 formants using Praat (Parselmouth)
        
        Research basis: People who stutter show vowel centralization
        (reduced vowel space area in F1-F2 plane)
        """
        try:
            snd = parselmouth.Sound(audio_path)
            formants_obj = snd.to_formant_burg(time_step=0.02, max_number_of_formants=4)
            
            # Extract formant trajectories
            f1_trajectory = []
            f2_trajectory = []
            f3_trajectory = []
            f4_trajectory = []
            
            for t in range(formants_obj.get_number_of_frames()):
                time = formants_obj.get_time_from_frame_number(t + 1)
                f1 = formants_obj.get_value_at_time(1, time)
                f2 = formants_obj.get_value_at_time(2, time)
                f3 = formants_obj.get_value_at_time(3, time)
                f4 = formants_obj.get_value_at_time(4, time)
                
                if f1 and f2:  # Only include valid frames
                    f1_trajectory.append(f1)
                    f2_trajectory.append(f2)
                    f3_trajectory.append(f3 if f3 else 0)
                    f4_trajectory.append(f4 if f4 else 0)
            
            # Calculate vowel space area (F1-F2 triangle for corner vowels)
            # Approximation: Use min/max bounds as proxy for vowel space
            if f1_trajectory and f2_trajectory:
                f1_range = np.max(f1_trajectory) - np.min(f1_trajectory)
                f2_range = np.max(f2_trajectory) - np.min(f2_trajectory)
                vowel_space_area = f1_range * f2_range
                
                # Normalize against typical values (rough estimate)
                typical_area = 400000  # Hz^2 (typical F1 range ~600Hz * F2 range ~1500Hz)
                normalized_area = vowel_space_area / typical_area
            else:
                normalized_area = 1.0
                vowel_space_area = 0
            
            formants_dict = {
                'f1': np.array(f1_trajectory),
                'f2': np.array(f2_trajectory),
                'f3': np.array(f3_trajectory),
                'f4': np.array(f4_trajectory)
            }
            
            summary = {
                'vowel_space_area': float(vowel_space_area),
                'normalized_vowel_space': round(normalized_area, 2),
                'is_centralized': normalized_area < VOWEL_SPACE_REDUCTION_THRESHOLD,
                'f1_mean': float(np.mean(f1_trajectory)) if f1_trajectory else 0,
                'f2_mean': float(np.mean(f2_trajectory)) if f2_trajectory else 0
            }
            
            return formants_dict, summary
            
        except Exception as e:
            logger.warning(f"Formant extraction failed: {e}")
            return {}, {'vowel_space_area': 0, 'normalized_vowel_space': 1.0, 'is_centralized': False}

    def _extract_voice_quality(self, audio_path: str) -> Dict[str, float]:
        """
        Extract voice quality measures using Praat
        
        - Jitter: Pitch period perturbation (>1% = instability)
        - Shimmer: Amplitude perturbation (>3% = instability)
        - HNR: Harmonics-to-Noise Ratio (<15 dB = breathy/hoarse)
        """
        try:
            snd = parselmouth.Sound(audio_path)
            
            # Pitch object
            pitch = snd.to_pitch()
            
            # Point process for jitter/shimmer
            point_process = parselmouth.praat.call(snd, "To PointProcess (periodic, cc)", 75, 600)
            
            # Jitter (local)
            jitter = parselmouth.praat.call(point_process, "Get jitter (local)", 0, 0, 0.0001, 0.02, 1.3)
            
            # Shimmer (local)
            shimmer = parselmouth.praat.call([snd, point_process], "Get shimmer (local)", 0, 0, 0.0001, 0.02, 1.3, 1.6)
            
            # Harmonics-to-Noise Ratio
            harmonicity = snd.to_harmonicity()
            hnr = parselmouth.praat.call(harmonicity, "Get mean", 0, 0)
            
            return {
                'jitter': round(float(jitter), 4),
                'shimmer': round(float(shimmer), 4),
                'hnr': round(float(hnr), 2),
                'is_jittery': jitter > JITTER_THRESHOLD,
                'is_shimmery': shimmer > SHIMMER_THRESHOLD,
                'is_low_hnr': hnr < HNR_THRESHOLD
            }
            
        except Exception as e:
            logger.warning(f"Voice quality extraction failed: {e}")
            return {
                'jitter': 0.0, 
                'shimmer': 0.0, 
                'hnr': 20.0,
                'is_jittery': False,
                'is_shimmery': False,
                'is_low_hnr': False
            }

    def _extract_pitch(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """Extract pitch contour (F0)"""
        try:
            pitch, _ = librosa.piptrack(y=audio, sr=sr, hop_length=320)
            pitch_contour = np.max(pitch, axis=0)
            return pitch_contour
        except:
            return np.zeros(len(audio) // 320)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # TRANSCRIPTION & UNCERTAINTY
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _transcribe_with_timestamps(
        self, 
        audio: np.ndarray
    ) -> Tuple[str, List[Dict], torch.Tensor]:
        """
        Wav2Vec2 transcription with word-level timestamps
        """
        inputs = self.processor(audio, sampling_rate=16000, return_tensors="pt")
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
        if DEVICE == "cuda":
            inputs["input_values"] = inputs["input_values"].half()

        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits

        predicted_ids = torch.argmax(logits, dim=-1)
        transcript = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        
        word_timestamps = self._extract_word_timestamps(predicted_ids[0], logits[0], transcript)
        
        return transcript, word_timestamps, logits

    def _calculate_uncertainty(self, logits: torch.Tensor) -> Tuple[float, List[Dict]]:
        """
        Calculate model uncertainty via entropy
        
        High entropy = model confusion = likely stutter or dysfluency
        """
        probs = torch.nn.functional.softmax(logits, dim=-1)
        
        # Global entropy score
        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)
        avg_entropy = torch.mean(entropy).item()
        
        # Low confidence regions
        max_probs, _ = torch.max(probs, dim=-1)
        try:
            max_probs_avg = max_probs.detach().cpu().float().mean(dim=0)
        except:
            max_probs_avg = max_probs.detach().cpu().float()
        
        low_conf_mask = max_probs_avg < CONFIDENCE_LOW_THRESHOLD
        low_conf_indices = torch.nonzero(low_conf_mask, as_tuple=False).squeeze(-1)
        
        mismatches = []
        if low_conf_indices.numel() > 0:
            timestamps = low_conf_indices.float() * 0.02
            last_t = -1.0
            for t in timestamps:
                t_val = float(t.item())
                if t_val - last_t > 0.5:
                    mismatches.append({"time": round(t_val, 2), "type": "low_confidence"})
                last_t = t_val
        
        return avg_entropy, mismatches

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ADVANCED STUTTER DETECTION ALGORITHMS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _detect_prolongations_advanced(
        self,
        mfcc_features: np.ndarray,
        spectral_flux: np.ndarray,
        speaking_rate: float,
        word_timestamps: List[Dict]
    ) -> List[StutterEvent]:
        """
        Enhanced prolongation detection with spectral flux analysis
        
        Criteria:
        1. Spectral correlation >0.9 (MFCC similarity)
        2. Duration >250ms (adaptive)
        3. Low spectral flux (minimal change)
        """
        events = []
        frame_time = 0.02
        
        min_duration = PROLONGATION_MIN_DURATION * (SPEECH_RATE_MIN / max(speaking_rate, 1.0))
        min_frames = int(min_duration / frame_time)
        
        n_frames = mfcc_features.shape[1]
        prolongation_start = None
        high_correlation_frames = 0
        
        for i in range(1, min(n_frames, len(spectral_flux))):
            corr = np.corrcoef(mfcc_features[:, i-1], mfcc_features[:, i])[0, 1]
            flux = spectral_flux[i] if i < len(spectral_flux) else 0
            
            # High correlation + low flux = prolongation
            if corr > PROLONGATION_CORRELATION_THRESHOLD and flux < np.percentile(spectral_flux, 30):
                if prolongation_start is None:
                    prolongation_start = i - 1
                high_correlation_frames += 1
            else:
                if prolongation_start is not None and high_correlation_frames >= min_frames:
                    start_time = prolongation_start * frame_time
                    end_time = i * frame_time
                    word_context = self._find_word_at_time(start_time, word_timestamps)
                    
                    events.append(StutterEvent(
                        type='prolongation',
                        start=round(start_time, 2),
                        end=round(end_time, 2),
                        text=word_context if word_context else '<prolongation>',
                        confidence=min(corr, 0.98),
                        acoustic_features={
                            'correlation': float(corr), 
                            'duration': end_time - start_time,
                            'spectral_flux': float(flux)
                        }
                    ))
                
                prolongation_start = None
                high_correlation_frames = 0
        
        return events

    def _detect_blocks_enhanced(
        self,
        audio: np.ndarray,
        sr: int,
        rms_energy: np.ndarray,
        zcr: np.ndarray,
        word_timestamps: List[Dict],
        speaking_rate: float
    ) -> List[StutterEvent]:
        """
        Enhanced block detection with ZCR analysis
        
        Blocks = silence gaps + low energy + low ZCR (voiced silence)
        """
        events = []
        frame_time = 0.02
        
        block_threshold = BLOCK_SILENCE_THRESHOLD * (SPEECH_RATE_MIN / max(speaking_rate, 1.0))
        silence_threshold = np.percentile(rms_energy, BLOCK_ENERGY_PERCENTILE)
        
        is_silent = rms_energy < silence_threshold
        silence_start = None
        silence_duration = 0
        
        for i, silent in enumerate(is_silent):
            if silent:
                if silence_start is None:
                    silence_start = i
                silence_duration += 1
            else:
                if silence_start is not None:
                    duration_sec = silence_duration * frame_time
                    
                    if duration_sec > block_threshold:
                        start_time = silence_start * frame_time
                        end_time = i * frame_time
                        
                        # Check if unnatural gap between words
                        if self._is_unnatural_gap(start_time, end_time, word_timestamps):
                            # Check ZCR: low ZCR = voiced attempt (articulatory block)
                            avg_zcr = np.mean(zcr[silence_start:i]) if i < len(zcr) else 0
                            is_voiced_block = avg_zcr < ZCR_VOICED_THRESHOLD
                            
                            events.append(StutterEvent(
                                type='block',
                                start=round(start_time, 2),
                                end=round(end_time, 2),
                                text='<voiced_block>' if is_voiced_block else '<silence>',
                                confidence=0.90,
                                acoustic_features={
                                    'duration': duration_sec,
                                    'avg_zcr': float(avg_zcr),
                                    'is_voiced': is_voiced_block
                                }
                            ))
                
                silence_start = None
                silence_duration = 0
        
        return events

    def _detect_repetitions_advanced(
        self,
        mfcc_features: np.ndarray,
        formants: Dict[str, np.ndarray],
        word_timestamps: List[Dict],
        transcript: str,
        speaking_rate: float
    ) -> List[StutterEvent]:
        """
        Multi-modal repetition detection using DTW + formant similarity
        
        Combines:
        1. Text matching (same word)
        2. MFCC DTW distance <0.15
        3. Formant trajectory similarity (if available)
        """
        events = []
        
        if len(word_timestamps) < 2:
            return events
        
        dtw_threshold = REPETITION_DTW_THRESHOLD * (max(speaking_rate, 1.0) / SPEECH_RATE_MIN)
        
        for i in range(len(word_timestamps) - 1):
            curr_word = word_timestamps[i]
            next_word = word_timestamps[i + 1]
            
            # Text-based matching
            if curr_word['word'].lower() == next_word['word'].lower():
                curr_start_frame = int(curr_word['start'] / 0.02)
                curr_end_frame = int(curr_word['end'] / 0.02)
                next_start_frame = int(next_word['start'] / 0.02)
                next_end_frame = int(next_word['end'] / 0.02)
                
                if curr_end_frame > mfcc_features.shape[1] or next_end_frame > mfcc_features.shape[1]:
                    continue
                
                curr_segment = mfcc_features[:, curr_start_frame:curr_end_frame].T
                next_segment = mfcc_features[:, next_start_frame:next_end_frame].T
                
                # DTW distance
                try:
                    distance, _ = fastdtw(curr_segment, next_segment, dist=euclidean)
                    normalized_distance = distance / max(len(curr_segment), len(next_segment))
                    
                    # Additional formant check if available
                    formant_similarity = 1.0
                    if formants and 'f1' in formants and len(formants['f1']) > 0:
                        formant_similarity = self._compute_formant_similarity(
                            formants, curr_start_frame, curr_end_frame,
                            next_start_frame, next_end_frame
                        )
                    
                    # Combined decision
                    if normalized_distance < dtw_threshold and formant_similarity > 0.7:
                        events.append(StutterEvent(
                            type='repetition',
                            start=round(curr_word['start'], 2),
                            end=round(next_word['end'], 2),
                            text=curr_word['word'],
                            confidence=min(0.95, 1.0 - normalized_distance),
                            acoustic_features={
                                'dtw_distance': float(normalized_distance),
                                'formant_similarity': float(formant_similarity)
                            }
                        ))
                except:
                    pass
        
        return events

    def _compute_formant_similarity(
        self,
        formants: Dict[str, np.ndarray],
        start1: int, end1: int,
        start2: int, end2: int
    ) -> float:
        """
        Compute formant trajectory similarity using cosine distance
        """
        try:
            f1_seg1 = formants['f1'][start1:end1]
            f1_seg2 = formants['f1'][start2:end2]
            f2_seg1 = formants['f2'][start1:end1]
            f2_seg2 = formants['f2'][start2:end2]
            
            if len(f1_seg1) == 0 or len(f1_seg2) == 0:
                return 1.0
            
            # Align lengths (simple interpolation)
            from scipy.interpolate import interp1d
            max_len = max(len(f1_seg1), len(f1_seg2))
            
            if len(f1_seg1) > 1:
                f1_interp1 = interp1d(np.linspace(0, 1, len(f1_seg1)), f1_seg1)
                f1_aligned1 = f1_interp1(np.linspace(0, 1, max_len))
            else:
                f1_aligned1 = np.full(max_len, f1_seg1[0])
            
            if len(f1_seg2) > 1:
                f1_interp2 = interp1d(np.linspace(0, 1, len(f1_seg2)), f1_seg2)
                f1_aligned2 = f1_interp2(np.linspace(0, 1, max_len))
            else:
                f1_aligned2 = np.full(max_len, f1_seg2[0])
            
            # Cosine similarity
            similarity = 1 - cosine(f1_aligned1, f1_aligned2)
            return max(0, similarity)
        except:
            return 1.0

    def _detect_voice_quality_issues(
        self,
        audio_path: str,
        word_timestamps: List[Dict],
        voice_quality: Dict[str, float]
    ) -> List[StutterEvent]:
        """
        Detect dysfluencies based on voice quality degradation
        
        High jitter/shimmer or low HNR indicates vocal instability
        """
        events = []
        
        # Global voice quality issues
        if voice_quality.get('is_jittery') or voice_quality.get('is_shimmery') or voice_quality.get('is_low_hnr'):
            # Map to word-level events (simplified: mark all words)
            for word in word_timestamps:
                events.append(StutterEvent(
                    type='dysfluency',
                    start=word['start'],
                    end=word['end'],
                    text=word['word'],
                    confidence=0.65,
                    voice_quality=voice_quality,
                    acoustic_features={
                        'jitter': voice_quality.get('jitter', 0),
                        'shimmer': voice_quality.get('shimmer', 0),
                        'hnr': voice_quality.get('hnr', 20)
                    }
                ))
        
        return events

    def _detect_anomalies(
        self,
        events: List[StutterEvent],
        features: Dict[str, Any]
    ) -> List[StutterEvent]:
        """
        Use Isolation Forest to detect anomalous acoustic patterns
        """
        if len(events) < 10:
            return events
        
        try:
            # Extract feature vectors from events
            feature_matrix = []
            for event in events:
                feat_vec = [
                    event.end - event.start,  # Duration
                    event.confidence,
                    event.acoustic_features.get('correlation', 0),
                    event.acoustic_features.get('dtw_distance', 0),
                    event.acoustic_features.get('avg_zcr', 0),
                ]
                feature_matrix.append(feat_vec)
            
            X = np.array(feature_matrix)
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # Fit Isolation Forest
            self.anomaly_detector.fit(X_scaled)
            predictions = self.anomaly_detector.predict(X_scaled)
            
            # Mark anomalies with higher confidence
            for i, pred in enumerate(predictions):
                if pred == -1:  # Anomaly
                    events[i].confidence = min(events[i].confidence * 1.2, 0.99)
                    events[i].acoustic_features['is_anomaly'] = True
        
        except Exception as e:
            logger.warning(f"Anomaly detection failed: {e}")
        
        return events

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # HELPER FUNCTIONS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    def _estimate_speaking_rate(self, audio: np.ndarray, sr: int) -> float:
        """Estimate speaking rate via onset detection"""
        onset_frames = librosa.onset.onset_detect(y=audio, sr=sr, hop_length=320)
        duration = len(audio) / sr
        if duration > 0:
            rate = len(onset_frames) / duration
            return max(SPEECH_RATE_MIN, min(rate, SPEECH_RATE_MAX))
        return SPEECH_RATE_TYPICAL

    def _extract_word_timestamps(
        self, 
        predicted_ids: torch.Tensor,
        logits: torch.Tensor,
        transcript: str
    ) -> List[Dict]:
        """CTC-aware word-level timestamp extraction"""
        word_timestamps: List[Dict] = []
        frame_time = 0.02

        if isinstance(predicted_ids, torch.Tensor):
            ids = predicted_ids.detach().cpu().numpy().tolist()
        else:
            ids = list(predicted_ids)

        try:
            token_strings = self.processor.tokenizer.convert_ids_to_tokens(ids)
        except:
            token_strings = [self.processor.tokenizer.decode([int(i)]) for i in ids]

        pad_token_id = getattr(self.processor.tokenizer, 'pad_token_id', None)
        boundary_markers = ('‚ñÅ', 'ƒ†', ' ')

        current_word = ''
        word_start_frame = None

        for i, (tid, tok) in enumerate(zip(ids, token_strings)):
            if pad_token_id is not None and int(tid) == int(pad_token_id):
                if current_word and word_start_frame is not None:
                    word_timestamps.append({
                        'word': current_word,
                        'start': round(word_start_frame * frame_time, 2),
                        'end': round(i * frame_time, 2)
                    })
                    current_word = ''
                    word_start_frame = None
                continue

            tok_str = tok if isinstance(tok, str) else str(tok)
            tok_norm = tok_str.strip()
            is_boundary = any(tok_str.startswith(m) for m in boundary_markers)

            for m in boundary_markers:
                if tok_str.startswith(m):
                    tok_norm = tok_str.lstrip(m)
                    break

            if is_boundary:
                if current_word and word_start_frame is not None:
                    word_timestamps.append({
                        'word': current_word,
                        'start': round(word_start_frame * frame_time, 2),
                        'end': round(i * frame_time, 2)
                    })
                if tok_norm:
                    current_word = tok_norm
                    word_start_frame = i
                else:
                    current_word = ''
                    word_start_frame = None
            else:
                if word_start_frame is None:
                    word_start_frame = i
                current_word += tok_norm

        if current_word and word_start_frame is not None:
            word_timestamps.append({
                'word': current_word,
                'start': round(word_start_frame * frame_time, 2),
                'end': round((len(ids) - 1) * frame_time, 2)
            })

        return word_timestamps

    def _find_word_at_time(self, time: float, word_timestamps: List[Dict]) -> str:
        """Find word at given timestamp"""
        for word in word_timestamps:
            if word['start'] <= time <= word['end']:
                return word['word']
        return ""

    def _is_unnatural_gap(
        self, 
        gap_start: float, 
        gap_end: float, 
        word_timestamps: List[Dict]
    ) -> bool:
        """Check if silence gap is mid-utterance (unnatural)"""
        before_words = [w for w in word_timestamps if w['end'] <= gap_start]
        after_words = [w for w in word_timestamps if w['start'] >= gap_end]
        return len(before_words) > 0 and len(after_words) > 0

    def _is_overlapping(self, time: float, events: List[StutterEvent]) -> bool:
        """Check if time overlaps with existing events"""
        return any(abs(e.start - time) < 0.5 for e in events)

    def _deduplicate_events_cascade(self, events: List[StutterEvent]) -> List[StutterEvent]:
        """
        Merge overlapping events with priority hierarchy
        Priority: Block > Repetition > Prolongation > Dysfluency
        """
        if not events: return []
        
        priority = {'block': 0, 'repetition': 1, 'prolongation': 2, 'dysfluency': 3}
        events.sort(key=lambda x: (x.start, priority.get(x.type, 4)))
        
        merged = []
        current = events[0]
        
        for next_event in events[1:]:
            if next_event.start < current.end:
                if priority.get(next_event.type, 4) < priority.get(current.type, 4):
                    current = next_event
                current.end = max(current.end, next_event.end)
            else:
                merged.append(current)
                current = next_event
        
        merged.append(current)
        return merged

    def _calculate_clinical_metrics(
        self, 
        events: List[StutterEvent], 
        duration: float,
        speaking_rate: float,
        features: Dict[str, Any]
    ) -> Dict:
        """
        Calculate comprehensive clinical metrics
        
        Based on:
        - Stuttering Severity Instrument (SSI-4)
        - Clinical severity classification standards
        """
        total_time = sum(e.end - e.start for e in events)
        freq = (len(events) / duration * 60) if duration > 0 else 0
        ratio = (total_time / duration * 100) if duration > 0 else 0
        
        # Count by type
        blocks = sum(1 for e in events if e.type == 'block')
        prolongations = sum(1 for e in events if e.type == 'prolongation')
        repetitions = sum(1 for e in events if e.type == 'repetition')
        dysfluencies = sum(1 for e in events if e.type == 'dysfluency')
        
        # Weighted severity (blocks are more severe)
        weighted_score = (blocks * 3.0 + prolongations * 2.0 + repetitions * 1.5 + dysfluencies * 1.0)
        
        # Formant-based adjustment
        vowel_centralization = features['formant_summary'].get('is_centralized', False)
        if vowel_centralization:
            weighted_score *= 1.2
        
        # Voice quality adjustment
        voice_issues = (
            features['voice_quality'].get('is_jittery', False) or
            features['voice_quality'].get('is_shimmery', False) or
            features['voice_quality'].get('is_low_hnr', False)
        )
        if voice_issues:
            weighted_score *= 1.1
        
        # Severity classification (SSI-4 inspired)
        if ratio < 2 and freq < 3:
            label = 'none'
        elif ratio < 5 or freq < 6:
            label = 'mild'
        elif ratio < 10 or freq < 12:
            label = 'moderate'
        elif ratio < 20 or freq < 20:
            label = 'severe'
        else:
            label = 'very_severe'
        
        # Confidence calculation
        avg_confidence = np.mean([e.confidence for e in events]) if events else 0.5
        
        # Clinical interpretation
        clinical_notes = []
        if blocks > 0:
            clinical_notes.append(f"{blocks} articulatory blocks detected")
        if vowel_centralization:
            clinical_notes.append("vowel space centralization observed")
        if voice_issues:
            clinical_notes.append("voice quality instability detected")
        
        return {
            'total_duration': round(total_time, 2),
            'frequency': round(freq, 1),
            'severity_score': round(ratio, 1),
            'weighted_severity': round(weighted_score, 1),
            'severity_label': label,
            'confidence': round(avg_confidence, 2),
            'num_blocks': blocks,
            'num_prolongations': prolongations,
            'num_repetitions': repetitions,
            'num_dysfluencies': dysfluencies,
            'clinical_notes': clinical_notes
        }

    def _event_to_dict(self, event: StutterEvent) -> Dict:
        """Convert StutterEvent to dictionary"""
        return {
            'type': event.type,
            'start': event.start,
            'end': event.end,
            'text': event.text,
            'confidence': event.confidence,
            'acoustic_features': event.acoustic_features,
            'voice_quality': event.voice_quality,
            'formant_data': event.formant_data
        }

    def _detect_language_robust(self, audio_path: str) -> str:
        """Multi-segment voting system for language identification"""
        logger.info("üïµÔ∏è Pro-LID: Analyzing audio structure...")
        try:
            lid_processor = AutoFeatureExtractor.from_pretrained(LID_MODEL_ID)
            lid_model = Wav2Vec2ForSequenceClassification.from_pretrained(LID_MODEL_ID).to(DEVICE)
            
            duration = librosa.get_duration(path=audio_path)
            segments = [(0, 10), (max(0, duration/2-5), 10), (max(0, duration-10), 10)]
            
            votes = []
            for off, dur in segments:
                if duration < off: continue
                try:
                    audio, _ = librosa.load(audio_path, sr=16000, offset=off, duration=dur)
                    if len(audio) < 16000: continue
                    
                    inputs = lid_processor(audio, sampling_rate=16000, return_tensors="pt").to(DEVICE)
                    with torch.no_grad():
                        logits = lid_model(**inputs).logits
                    votes.append(lid_model.config.id2label[torch.argmax(logits).item()])
                except: pass
            
            del lid_model
            if DEVICE == "cuda": torch.cuda.empty_cache()
            
            if not votes: return 'eng'
            final = Counter(votes).most_common(1)[0][0]
            logger.info(f"‚úÖ Detected: {final} (Votes: {votes})")
            return final
            
        except Exception as e:
            logger.error(f"LID Error: {e}")
            return 'eng'

    def _activate_adapter(self, code: str):
        """Activate language-specific adapter"""
        if code not in self.loaded_adapters:
            try:
                self.model.load_adapter(code)
                self.loaded_adapters.add(code)
            except:
                logger.warning(f"Adapter {code} failed, using fallback")
                code = 'eng'
        self.processor.tokenizer.set_target_lang(code)
        self.model.load_adapter(code)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# USAGE EXAMPLE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if __name__ == "__main__":
    # Initialize detector
    detector = AdvancedStutterDetector()
    
    # Analyze audio file
    results = detector.analyze_audio("path/to/audio.wav", language="english")
    
    # Print comprehensive report
    print("=" * 70)
    print("STUTTERING ANALYSIS REPORT")
    print("=" * 70)
    print(f"Transcript: {results['actual_transcript']}")
    print(f"Severity: {results['severity']} ({results['mismatch_percentage']}%)")
    print(f"Speaking Rate: {results['speaking_rate_sps']} syllables/sec")
    print(f"Confidence: {results['confidence_score']}")
    print(f"\nVoice Quality:")
    print(f"  Jitter: {results['voice_quality_metrics']['jitter']}")
    print(f"  Shimmer: {results['voice_quality_metrics']['shimmer']}")
    print(f"  HNR: {results['voice_quality_metrics']['hnr']} dB")
    print(f"\nFormant Analysis:")
    print(f"  Vowel Space: {results['formant_analysis']['normalized_vowel_space']}")
    print(f"  Centralized: {results['formant_analysis']['is_centralized']}")
    print(f"\nDetected Events:")
    for event in results['stutter_timestamps']:
        print(f"  [{event['start']}-{event['end']}s] {event['type']}: {event['text']}")